bayesian_statistics
===================
- Category: Learning
- Tags: 
- Created: 2024-12-20T18:05:42-08:00

## **01 - GOLEM OF PRAGUE**

``https://www.youtube.com/watch?v=FdnMWdICdRs``
- Thinking about how science influences statistics (an applied form of mathematics)
- Bayesian inference vs. frequentism is a common debate in stats courses
	- Won't discuss it though. Looking at causal inference instead. Bayesian debate is mostly settled
		- Science before statistics.
		- For statistical models to produce scientific insight, they require additional scientific (causal) models where changing a var changes another
		- The reasons for a statistical analysis are not found in the data but in the causes of the data. Numbers mean nothing without context.
			- Data cannot be extracted from causes alone (requires testing.) Likewise, causes cannot be extracted from data alone. No causes in; no causes out.
		- What is causal inference?
			- More of an association between variables
			- "Causal inference" is prediction of intervention
				- Knowing a cause means being able to predict the consequence s of an intervention by that cause.	
				- Ex. tree swaying outside in the wind is caused by wind. The two are associated and one can cause the other, but we're usually sure it's the wind causing the swaying.
			- "Causal imputation" is imputation of missing observations
				- What does this mean? Knowing a cause lets you ask "what if xyz was done differently?" (construct unobserved counterfactual outcomes)
			- Causal inference is related to description (of population) and design (of project)
				- These all rely on some scientific model
- DAGs (Directic Acyclic Graphs)
	- Essentially highly abstracted causal models - the only info in a DAG are variables and one-way causal relationships.
	- All a DAG defines is relationships - no more, no less. Used to clarify thinking.
	- Analyze this to produce appropriate statistical models.
	- "What can we decide, without additional assumptions?"
	- See video example
		- X -> Y
		- C -> X, Y
		- A -> C, X
		- B -> C, Y
		- X influences Y, which is what we want to test. Other variables influence these, though.
			- X and B are competing causes of Y
			- A is an influence of the treatment X
			- C is a common cause of X and Y - a compound to control for
	- DAGs let us ask questions and consider different models
		- Which control variables? DONT ADD EVERYTHING
		- How do we test and refine the causal model?
	- Also, they let us focus more on science rather than data
- Golems
	- The course's name for statistical models
	- Named after golem of Judaism. It's a hollow puppet that performs a purpose regardless of intent.
		- Likewise, statistical models have no inherent purpose.
		- They are also powerful, intent-agnostic, and dangerous
	- Quick rant about frequentist statistical models
		- Incredibly limiting
		- Focus on rejecting null hypothesis
		- Relationship between research and test unclear
		- Industrial framework
		- "Follow the flowchart" approach not ideal
	- Lots of studies revolve around complex systems - null models are rarely not feasible
		- Null pop. dynamics?
		- Null phylogeny?
		- Null social network?
		- Problem: many processes produce similar distributions
	- Example: how does DNA evolve? Neutral evolution vs. selection?
		- The two hypotheses lead to distinct process models, but overlapping statistical models
	- We're going to start with DAGs, turn them into generative causal models, then write statistical models to analyze synthetic data, then introduce real data.
		- Start with abstracted view
		- Generate synthetic data based on our abstracted DAG view
		- Write statistical models to analyze synthetic data
		- Use statistical models on real data
			- Like training an ML model!
	- General process:
		- Make a DAG, select "adjustment set" (relevant variables)
		- Need generative model of causal model to design/debug our code
		- Need a strategy to derive estimate and uncertainty from finite data
			- Easiest approach: bayesian data analysis. It's easiest to use because Bayesian models are generative - simulate the data yourself
- Owls
	- Common joke - step 1 is draw circles, step 2 is draw the rest of the owl
		- But sometimes it feels like this when teaching computational materials
	- We're interested in documenting our workflow.
		- Understand what you're doing
		- Reduce error, allow others to analyze your work better and find problems.
		- Respectable scientific workflow
	- 1) Theoretical estimand - what are we even doing in the study?
	- 2) Scientific (causal) model(s) - start as a DAG, but needs to end up being generative.
	- 3) Use (1) and (2) to build statistical models
	- 4) Simulate from (2) to valdiate that (3) yields (1)
	- 5) Analyze real data

## **02 - GARDEN OF FORKING DATA**

``https://www.youtube.com/watch?v=R1vcdhPBlXA``
- Imagine you're trying to model the proportion of land to water on the surface of the earth.
	- We can estimate this by using a model globe and tossing it, then seeing which type of surface (land/water) lands upright. Eventually, this should converge on the proportion.
- Here's our general workflow.
- 1) Define generative model of the sample
	- Start with DAG style - how do variables work with one another?
		- Which variables are we even working with?
			- p - proportion of water (our goal to figure this out)
			- n - number of tosses
			- l - number of land observations
			- w - number of water observations
	- n influences W and L - the more tosses you do, the values increase/decrease.
	- p influences W and L - as the proportion changes, the number of W/L changes.
		- Intervening on one of these values will influence the values of W and L
	- To make this generative, we need to define some function. W,L = f(p, n)
		- This can be really any function you want, but it needs to reflect the scientific scenario.
- 2) Define a specific estimand
	- The globe being tossed is our estimand
- 3) Design a statistical way to produce estimate
	- Bayesian data analysis - simple, humble way to do this.
	- "For each possible explanation of the sample (proportion of water), count all the ways the sample could happen."
	- "Explanations with more ways to produce the sample are more plausible."
	- Imagine a garden of forking paths. There's many datasets that can arise - think about all the different ways things can happen and certain decision points.
		- Ex. doing infinite datasets of the globe's infinite proportion in infinite tosses would be tough. Instead, let's do a 4 sided die and toss it 3 times for WLW.
		- For a D4 globe with fully covered sides, there's 5 possible globes (representing a abstracted subset of the real globe's proportion.)
			- LLLL
			- WLLL
			- WWLL
			- WWWL
			- WWWW
		- For each example proportion of W:L, find how many ways our given sample could arise.
			- Globe	 W | L | W
			- LLLL - 0 * 4 * 0 = 0 ways (there were 2 W in sample)
			- WLLL - 1 * 3 * 1 = 3 ways
			- WWLL - 2 * 2 * 2 = 8 ways
			- WWWL - 3 * 1 * 3 = 9 ways
			- WWWW - 4 * 0 * 4 = 0 ways (there was 1 L in sample)
				- See Forking Path diagram in video - useful to visualize
		- We can see that WWWL has more ways to produce - but the sample isn't very large, so there isn't much support to claim that the glopbe is WWWL.
			- This is a good feature of an estimator - not overconfident in small samples
		- We just produced a Bayesian analysis!
		- Now, let's do some Bayesian updating. Increase the size of the sample (roll again), so we now work with WLWW.
			- We can just multiply our values by the number of ways that the new sample can happen.
			- Globe	 WLW | W
			- LLLL - 0   * 0 = 0 ways
			- WLLL - 3   * 1 = 3 ways
			- WWLL - 8   * 2 = 16 ways
			- WWWL - 9   * 3 = 27 ways
			- WWWW - 0   * 4 = 0 ways
		- We can see the values starting to seperate a bit.
		- What about larger samples, like WLWWWLWLW? How can we express this mathematically?
			- Globe	 W | L | W | W | W | L | W | L | W
			- LLLL - 0 * 4 * 0 * 0 * 0 * 4 * 0 * 4 * 0 = 0^6 * 4^3 = 0 ways
			- WLLL - 1 * 3 * 1 * 1 * 1 * 3 * 1 * 3 * 1 = 1^6 * 3^3 = 27 ways
			- WWLL - 2 * 2 * 2 * 2 * 2 * 2 * 2 * 2 * 2 = 2^6 * 2^3 = 512 ways
			- WWWL - 3 * 1 * 3 * 3 * 3 * 1 * 3 * 1 * 3 = 3^6 * 1^3 = 729 ways 
			- WWWW - 4 * 0 * 4 * 4 * 4 * 0 * 4 * 0 * 4 = 4^6 * 0^3 = 0 ways
		- So our equation is W,L = (4p)^w * (4-4p)^l
		- As our samples start to get larger, the numbers of ways that things can happen will also increase exponentially. Better to convert it to probability.
			- How do we do this? Simple. Add all the possibilities up, then divide each possibility value by the total to get the percentage.
			- This collection of (proportion) probabilities is called the "posterier distribution"
- 4) Test (3) using (1)
	- At this point, we have designed an optimal Bayesian estimator (if the model is correct.)
		- What does this mean? So long as our generative model is correct/representative of real data, our estimates for likelihoods of p are optimal given generated data.
	- We want to test before we estimate and summarize.
		- 1) Code a generative simulation
		- 2) Code an estimator
		- 3) Test (2) with (1)
	- We can do 4-sided globe, 10-sided, 20-sided, etc.
		- But we can't do infinite-sided. This is the limitation. So how do we get our maximum?
		- Let's think about it logically.
		- The globe is a polyhedron with infinite sides
		- The posterior probability of any "side" p being water is proportional to:
			- (p^w)((1-p)^l) 
		- Now, all we need to do is normalize this and we get our Beta distribution - a single continuous function, instead of lines.
			- Beta distribution made up of of normalizing constant * relative number of ways to observe sample.
				- In this case, relative number of ways to observe sample is the function we calculated earlier - (p^w)((1-p)^l)
				- We then normalize this value to turn it into a probability in correspondence to the total number of possibilities.
					- Shape of the curve doesn't depend upon the normalizer at all. That's all chosen by our original function - normalizer just normalizes.
				- Beta distribution normalizes to density - 0.0 upward
				- Forms a curve when graphed. This curve starts to narrow and become higher as more samples as we update
	- A few lessons
		- 1) There's no minimum sample size. This is good, but unlike frequentist probability practices. Bayesian estimators work with small samples - it's just gonna be a pretty flat curve.
		- 2) Shape embodies sample size. This is really nice - we update the shape of the post. dist. when updating the samples. Also, see above.
		- 3) No point estimate. Use the whole distribution every time. Don't use mean/mode/whatever - the whole distribution, or a subset range of it, is relevant. THE DISTRIBUTION IS THE ESTIMATE.
		- 4) No one true interval. Intervals communicate shape of posterior, but the choice is pretty much arbitrary.
- 5) Analyze real sample, summarize
	- Now that we have our posterior distribution (a type of estimate), now what?
	- If you can do a single calculation on one point (proportion in this case), you can and must do it to every point as you randomly sample to create a new doistribution.
		- But to do this, we need to sample from the posterior distribution.
			- Usually, this requires integral calculus. Thankfully, we can approximate by sampling from the beta distribution. 
	- Video example: sample from posterior distribution. Then simulate the sampling again. Then plot. This makes a curve that accounts for uncertainty.
	- Sampling is fun and easy!
		- Sample from posterior, compute desired quantity for each sample, profit. Much easier than doing integrals.
		- Turn calculus problem into a data summary problem!
		- Markov Chain Monte Carlo (MCMC) produces only samples, anyways
- Notes on "Bayesian Modesty"
	- No guarantees except logical
	- Probability theory is a method of logically deducing implications of data under assumptions of your choice.

## **03 - GEOCENTRIC MODELS**

``https://www.youtube.com/watch?v=tNOu-SEacNU``
- Last time, we introduced basic logic of Bayesian inference
- This time, we're talking about expanding our Bayesian logic
- Example given - to explain Mars apparently "wandering", the geocentric model of the solar system was introduced
	- This worked well to predict the way that Mars would appear, but didn't really capture the mechanics of the solar system
	- Gauss took a stab at making a solar model and came up with normal error and least-squares estimation
		- We typically call his error distribution the "Gaussian distribution"
- This brings us to the topic of linear regressions
	- Like geocentric model, they describe associations and make predictions, but mechanistically wrong.
		- The universe is not comprised of perfect linear relationships. 
 		- The geocentric model is not inherently useless - it's just not a good generative model to represent the solar system. Good for planetariums, tho.
			- Linear regressions are powerful golems, but need appropriate usage
	- Lin. reg. is Gaussian by nature. We will use the Gaussian error model. 
		- Gauss defined very general conditions for how error creeps into observations
		- Gaussian distribution - there's a bunch of little errors that get added together
		- Ex. football field with lots of people on a line, heads = move right, tails = move left 
			- Over time, this will resemble a Gaussian distribution.
			- Why? Why do natural forces attract Gaussian (normal) distributions?
				- Fluctiations generally add and result to the more possible outcome.
				- Ex. more ways for people to end up in the middle than there are for people to end up far away.
- Two main arguments for why Gaussian (normal) distributions emerge
	- Generative - summed fluctuations tend towards normal distribution
	- Inferential - for estimating mean and variance, Gaussian distribution is least informative.
		- Often the best - all it does is give us a mean and variance. No more, no less.
		- A variable doesn't need to be normally distributed for normal model to be useful. It's a machine for estimating mean/variance.
- Remember our workflow. Let's update it.
	- 1) State a clear question/goal/estimand
	- 2) Sketch causal assumptions. Scientific model.
	- 3) Use the sketch (2) to define a generative model
	- 4) Use generative model (3) to build an estimator
	- 5) Validate model. Go back to (2) and start from there again if needed.
	- 6) Analyze real data.
- Ex. height vs. weight measurements
	- 1) State a clear question/goal/estimand
		- Describe association between weight and height
		- Make sure to state assumptions. We're only looking at adult weight/height 
	- 2) Sketch causal assumptions. Scientific model.
		- H -> W
			- Can also write this as W = f(H)
			- "Weight is some function of height"
			- We can quickly speculate about our generative model
				- Dynamic - incremental growth of organism, both H and W derive from growth pattern, Guassian variation result of summed fluctutations
				- Static - changes in height = changes in weight, but no mechanism. Gaussian variation result of growth history
		- But what about uncertainty? H -> W, U -> W
			- W = f(H, U)
	- 3) Use the sketch (2) to define a generative model
		- Start simple - W = Beta * H + U
			- "Weight is some proporion of H plus some uncertain factor"	
		- Let's also introduce some good conventions. When describing models...
			- List the variables 
			- Define each variable as a determninistic or distributional function of the other variables
		- So let's describe our variables now. i is the index for a single data point, = is "deterministic" and ~ is "distributed as"
			- Wi = Beta * Hi + Ui
			- Hi ~ Normal(0, sigma)
			- Ui ~ Uniform (130, 170)
			- Describe in this order with words, but for code, you need to describe the small components before your general statement.
	- 4) Use generative model (3) to build an estimator
		- When we build our stat. model, we're going to use some stuff that wasn't used in the generative model, since they don't typically follow the same shape. 	
			- Remember, this is a linear regression. The process is like last time with the globe, only now, we're converging on the parameters for a lin. reg. instead of a proportion of globe.
			- We'll make our statement for Bayesian updating our priors. For now, let's first define the statistical model.
				- E(Wi|Hi) = alpha + beta * Hi
					- "Our estimate for Wi given Hi is intercept alpha plus slope beta times Hi
					- Wi = average weight conditional on height
			- So what's our posterior? Remember, we're trying to find distributions for a specific line. 
				- Alpha and beta define the line for a specific data point (expected weight for each height)
				- Pr(alpha, beta, sigma | Hi, Wi) = (Pr(Wi | Hi, alpha, beta, sigma) * Pr(alpha, beta, sigma))/Z
					- Break this down intosmaller parts. What does this yucky statement even mean?
					- Pr(alpha, beta, sigma | Hi, Wi) -> We are trying to find a multivariable posterior distribution.
						- Post. dist. = the only estimator in Bayesian inference
						- In the past, we estimated the posterior distribution of the proportion of water to land.
							- This took our previous distribution and multiplied each proportion by the relative number of ways to achieve the new sample.
							- With WLW and a new W observation, It was (WLW possibilities) * (new possibilities to get W per each proportion
						- Now, we're taking multiple previous distributions and multiplying them by the new number of ways to achieve our 
		- Let's talk about priors real quick.
			- When doing the globe example, we started with 2 W and 1 L. We know that there's water and land on the globe, so our starting sample reflects reality already.
			- What about when we start from scratch? We need some value to start at - or rather, a distribution. Up to you to choose.
- At this point, I must be honest - I'm confusing myself a bit. So let's go from the beginning and take it slowly.
	- 1) State a clear question/goal/estimand
		- We want to describe the association between weight and height.
		- We're only looking at adult weight and height, so filter for any data points over the age of 18.
	- 2) Sketch causal assumptions. Scientific model.
		- In our globe example, there was one unobserved variable - the proportion of water to surface on the globe. This influenced the number of W/L observations. 
			- Now, we've got height influencing weight, but we've also got some other factors that we don't know about.
			- We'll call height H and the unclear stuff U. Our predictor function is loosely defined as ``W = f(H, U)`` 
	- 3) Use the sketch (2) to define a generative model
		- Looking at the data, we can also kind of see a linear relationship, so we'll model a linear regression.
			- The real equation probably looks something like W = beta * H + U
			- Let's also define what our variables will look like. Some variables are "random" (from a given distribution), some variables are deterministic functions of others 
				- ``Wi = B*Hi + Ui`` -> W is a function of other variables. Wi = beta(slope) * Hi + Ui
				- ``Ui ~ Normal(0,sigma)`` -> Ui is a normal distribution. **at least when we're generating.**
				- ``Hi ~ Uniform(130, 170)`` -> Hi is a uniform distribution from lower height to higher height
				- Beta is the slope that we're trying to estimate. Hi is observed. We don't give beta a value until we set priors.
	- 4) Use generative model (3) to build an estimator
		- Our estimator is going to resemble our previous generator. This time, however, we're trying to find certain unobserved values. Beta and Ui (alpha) were unobserved, 
			- ``E(Wi | Hi) = alpha + beta * Hi`` -> Read as "our estimate of Wi given Hi follows the following function:"
				- **KEY IDEA:** Our estimator is the final product. We're trying to complete the estimator using posterior distributions. 
		- Now we need to build our posterior distribution.
			- A couple things are different this time, though
				- We're doing a multivariable posterior distribution. It's still one distribution, but you can think of it as multiple one-variable distributions. Kind of.
				- Take a look at the equation in the video at ``38:30``
					- The left hand side is the posterior probability of a given line (as in the aforementioned multivariable distribution)
					- The left hand numerator on the right hand side is our "garden of forking data" - number of ways to get our 
					- The right hand numerator on the right hand side is our prior - previous posterior distribution
					- Z is our normalizing constant. Don't worry about it for now. In fact, you can kind of think of the numerator alone as the "number of ways the line could happen" like previously
						- Posterior = prior * number of new ways for sample to arise / normalizing constant
				- Normally, we don't write it like this. We can predict that Wi will converge on Gaussian
					- We write it as Wi ~ Normal(mu sub i, sigma), where mu sub i = alpha + beta * Hi. **This is our statistical model** that we're trying to posterior distribution-ify.
		- Choosing priors
			- We need to start with something when we have no priors. So we'll choose distributions based on what's scientifically justifiable. Up to preference.
				- Choose your priors *softly*. Weight generally goes up with height, when W=0 H=0, etc.
			- Think about our statistical model. Wi ~ Normal(mu sub i, sigma), where mu sub i = alpha + beta * H
				- Choose a prior for alpha, beta, and sigma. These are the unobserved values that we're making a distribution for.
			- Priors matter a lot in complex models. For simple models that can correct themselves easily, not as much.

## **04 - CATEGORIES AND CURVES**

``https://www.youtube.com/watch?v=F0N4b7K_iYQ``
- Short review
	- The geocentric model was an example where it produced good results but wasnt representative of the actual mechanics
	- Kind of similar to the linear regression. Useful to model things that aren't necessarily linear in nature
		- McElreath states it well - a "statistical scaffold" - used to get at statistical estimand, but need to design the linreg with an external, non-linear model in mind.
		- Like the geocentric model, linreg can approximate anything! So we need to design it with a specific owl in mind
- What's new in this lesson?
	- 1) How we draw the statistical models
		- When we have multiple estimatands, it's useful to use multiple estimators.
		- This is why it's important to make a good generator.  
	- 2) How we process the results
		- Often, the estimate we want is not going to show up directly in a table.
		- Need to postprocess multiple values from the posterior distribution.
- What are the new tools introduced in this lesson?
	- Categories
		- As in categorical variables
		- Usually show up as indicators or index variables
	- Curves
		- Linear models can sometimes do extralinear (curved) things.
		- Doesn't mean linear models are obsolete - linear models can handle both
- Categories
	- Lots of causes that are not continuous - categories are discrete, unordered types
	- How do we get around this? Ideally, we want to fit a seperate line (unique distribution) for each category
	- Imagine that we're back in our adult height/weight test. What if we have a data value for male vs. female in each?
		- FIRST! Add it to our DAG. 
			- Before: 	``H -> W``
			- Now: 		``H -> W and S -> H, W``
			- Think: how are H, W, and S causally related? How are they statistically related?
				- Important to ask these questions. If we look at the data, it's not clear whether ``H -> W`` or ``W -> H``, only that there's some relationship.
					- We know scientifically that gaining weight doesn't make you taller. But looking at only data doesn't support this.
				- Interesting note - sex influences weight in two ways - directly, and indirectly (through height.) Remember that there's more than one source of influence.
			- ``H = f sub H (S)`` - "H is a function of S"
			- ``W = f sub W (H, S)`` - "W is a function of H and S"
			- Notice that we don't necessaily *have* to write out the unobserved influences U. Generally it's implied, although you'll want to write out these influences if they're shared.
				- See turtle and lizard example. ``T -> S``, but ``T -> W`` (think food availability, levels of fat storage, etc.)
					- We know it's a compound, so it's not safe to ignore the temperature anymore
		- Think about how we're going to add this to our program
			- Depending on the sex value, use different generator stats and priors.
			- Can use indicator variables (bool for each option) or index variables (one int with multiple values)
				- Natural to see which of these is better coding style. Also better for specifying unique priors and extension to multi-level models
				- Ex. if we want to do color categories, unordered set alpha comprises of ``[a1, a2, ..., ai]`` where each value is a color.
				- Then, when selecting variables, choose from an array for whatever value you want based on the index value of given alpha
				- Ex. ``S[i]`` corresponds to the sex of the i-th person. For person i, we get their index (corresponding to sex category), then get our variable (in this case, intercept alpha)
				- If you use indicator variables, one value becomes the "default" and others become "adjustments" as opposed to everything being on the same field from the getgo.
			- We can find the difference through generative testing. Create data sets with simulated people for each prior and compare the mean weights.
		- We have our posterior distributions. Now what?
			- Remember - posterior distribution is for the average weight. For predicted weights, we need to include standard deviation into our distribution for the posterior predicted weights.
		- We're looking for the difference in posterior distributions, not in the means. So we need to find the difference in distributions. Need to compute contrast!
			- Never legitimate to compare overlap in distributions. You can't tell if things are different or the same based on distribution overlaps.
			- Must compute the **contrast distribution**. This is the actual scientific estimate we're after.
		- What about individual people? (Not the average of infinite people)
			- Take a large number of samples from both predicted posteriors, then take the difference between those two groups.
				- Think of it like so - sample a man and a woman. Find the difference in weight. Do this again and again, n times. This forms a distribution.
				- Negative value indicates woman is heavier than man because the difference is man - woman in weight.
	- This was the causal effect of S on W. Now, how about the direct causal effect of S on W? (Note: this is a distributional estimate)
		- Direct effect = How ONLY sex affects weight. No indirect influence through height - we need to "block" association through H. This means stratify by H.
		- Achieve this by "centering" H - instead of ``mu sub i = alpha + beta * H``, we subtract the average height from our observed height.
			- Makes it so alpha is the average weight of a person with average height
			- With the men and women example, this shows us that the slopes are nearly the same (but not identical!)
		- Then, for each height, simulate individuals and look at the difference in simulated weights. This gives us a posterior distribution in expected weight difference for each height.
			- This gives us the bow tie shape in the video. That's our direct causal effect.
- Curves
	- Not all data is linear. A lot of it can be curved.
	- Like if you look at the data for the full human lifespan, ``H -> W`` is clearly not linear
	- Linear models can easily fit curves, but it's not mechanistically representative.
		- But we knew linreg was like that already. Just have to use it wisely.
	- Technically, 	the model is still linear because it's an additive function of parameters
	- Two popular strategies of fitting curves
		- 1) Polynomials. "Awful."
			- Mu sub i = alpha + beta sub 1 * x sub i + beta sub 2 * x sub i
			- Lots of symmetries that are undesirable. Not scientifically reasonable.
			- Lots of "explosive uncertainty" at the edges
				- See video example for this. There's a little bit at each end of the curve which isn't scientifically justified.
			- Polynomials don't "smooth" the curve locally (don't determine the curve by looking at the curve in dense regions) - only global (accounting for EVERY point)
			- Parabolas **must** curve, and so they do - even when there isn't a very clearly supported fit and something else would do better
				- See video example - a curve fits the data perfectly, but it's not very clearly mechanistically approrpriate.
				- Lots of assumptions are made with a polynomial model that are undesired
		- 2) Splines and generalized additive models. "Less awful."
			- Splines are built from many local functions, then smoothed together to make a single function.
			- Linear models but with some "synthetic variables" (??? sound scary)
			- $\mu_i = \alpha + w_1\beta_i,1 + w_2\beta_i,2 + w_3\beta_i,3 + ...$
				- ``w`` is the weight of each point. "Like slopes."
				- B is a "spline shape" - synthetic, choose this to determine shape in a particular region.
					- Think of this as a coordinate on the x axis. "B values turn on weights in different regions"
			- Think of multiple "basis functions" adding up to give you a y value.
				- Can make very non-linear functions from linear pieces
			- Adding scientific information helps.
				- Ex. Average weight only increases with height, height increases but levels off with height, etc.
			- Ideally, statistical models have some form as a scientific model, which splines do.
				- Ex. Think phases of human growth. Infancy, childhood, puberty, adulthood all have different growth behaviors
		
## **05 - ELEMENTAL CONFOUNDS**

``https://www.youtube.com/watch?v=mBEA7PKDmiY``
- Certain variables can be correlated with one another without one causing the other. 
- Reminder: Direction of course
	- We have an estimand - parameter being estimated.
	- To make this, we need an estimator - our "set of instructions on how to assemble ingredients (parameters)"
	- This estimator produces an "estimate"
		- Sometimes, the estimate is inaccurate because things happen during the recipe.
		- Good statistical recipes of parameters must defend against confounding.
- There's four of "ye olde elemental confounds" that we'll look at
	- 1) "The Fork"
		- ``X <- Z -> Y``
		- Where Z is a "common cause"
		- When observing X and Y, they appear associated. 
			- Could say that Y "is not independent of" X. This is indicated by the upside down T with a line through it.
			- If you know Y or X, you know something about the other one.
		- Once stratified by Z, there's no association.
			- Meaning that X and Y *are* independent for each (shared) level of Z. 
			- Write this as $Y \perp X | Z$
			- Simulate Z first, then simulate X and Y based off Z - maybe offset by a coefficient of Z.
			- Sample Z with Bernoulli trials, then generate X and Y seperately with norm distributions. See discrete/continuous examples.
		- Ex. Why do regions of the USA with higher rates of marriage also have higher rates of divorce?
			- Estimand: Causal effect of marriage rate on divorce rate
			- Scientific model
				- Also heavily related: median age of marriage. We have some data regarding that as well.
				- Scientifically, we know that age of marriage has some effect on both marriage rates and divorce rates. ``M <- A -> D``
					- Break the fork by stratifying by A. We need to break the fork to estimate the causal effect of M on D.
					- How do we do this, actually? It's easy where A is discrete (like with Z being {0, 1} in the last example), but it's harder when A is continuous
					- It's the same, really. For every value of A, we look at the assocation between M and D. We just need a continuous function to tell us the output.
			- Statistical model
				- $\mu_i=\alpha+\beta_M * M_i + \beta_A * A_i$
				- Also think of it as $\mu_i=(\alpha + \beta_A * A_i) + \beta_M * M_i$
					- We're stratifying by A by combining it with our intercept. 
					- Remember - we're building an *estimator*. The betas refer to two distributions (starting as normal, but they eventually become our multivariable posterior.
				- Ok, so we have our equations. Remember that we can transform measurement scales arbitrarily, so long as we transform them back when done.
				- Standardizing variables is real handy when working with linear regressions. Not necessary, but helps with priors and makes computer more efficient.
					- Standardize by subtracting the mean and dividing by the std. dev. In the example, our prior std. devs are 10, which is "very flat" (unconfident, so to speak)
					- $D_i \sim Normal(\mu_i, \sigma)$
						- Remember - we're seeing how marriage rates influence divorce rates, and stratifying by age to avoid confounding.
					- $\mu_i=\alpha+\beta_M * M_i + \beta_A * A_i$ 
					- $\alpha \sim Normal(0, 10)$
					- $\beta_M \sim Normal(0, 10)$
					- $\beta_A \sim Normal(0, 10)$
					- $\sigma \sim Exponential(1)$
						- Exponential distribution curves out - high density as approaches 0. Like a typical std. dev
				- First, we take our data and standardize it.
				- Then, let's form our posteriors with quap. 
				- Then, let's graph them and compare with R. useful to do so on a "caterpillar plot" - think top down view of posteriors.
				- The slopes and whatnot are alright, but they aren't really great at indicating some change. We need to introduce interventions.
					- $p(D|do(M))$, meaning the distribution of D when we intervene ("do a certain") M.
						- This implies manually reaching in and moving M around without caring about A. Mutilation!
						- Think of it as ``A -> M, D, M -> D`` when no intervention but ``M, A -> D`` when we do(M)
				- How do we do this? First, sample from our statistical model w/ posteriors that we just generated.
					- NOT A NEW MODEL! We're just testing out how the model behaves when we intervene.
				- Then, we create our own "made-up" mean for M and simulate D for our intervened M and samples of A.
					- Ex. simulate D for M=0. Since we standardized our data, this is just the sample mean.
					- Ex. simulate D for M=1. Now the mean of M is 1 std. dev away. 
					- We compare the two of these by subtracting one distribution from another to find the difference upon changing. 
						- In the example, the difference is centered on 0. *Could* be large or small change (we don't know, since our result is a distribution.) 
				- In the example - what about $p(D|do(A))$? When we intervene on A, how does D change?
					- How do we go about intervening on A? There's no arrows to delete going into A.
						- Solution - fit new model that ignores M, then simulate any intervation you want.
					- Why does this work? We've already kind of been doing this. It's intuitive that changing A will change D, but that's because it pipes through M.    
	- 2) "The Pipe"
		- ``X -> Y -> Z``
		- Very similar in structure to the Fork. 
		- X and Y are associated becaus eof the influence transmitted through Z. 
		- Once we stratify by Z, there's no association.
			- $Y \perp X | Z$ - "Y is independent from X conditional of Z"
		- How is it different from the Fork?
			- Z isn't a common cause - no effect on X at all.
		- Toy sim: simulate X, then Z based off X, then Y based off Z.
			- When stratifying by a discrete Z, we see there's no association between X and Y. "When we know Z, knowing X doesn't tell us anything about Y."
		- Example: plant growth experiment. 100 plants, half treated with anti-fungal. Measure growth and fungus.
			- Estimand: Causal effect of treatment on plant growth. We're trying to see how our treatment of anti-fungal stuff affects plant growth.
			- Scientific model in abstract form
				- ``F, H0 -> H1`` - "The initial plant height and amount of fungus influences the final plant height after treatment/growth"
				- ``T -> F, H1`` - "The treatment has some effect on the amount of fungus and on the final plant height (positive or negative)"
			- Our estimand is the *total* causal effect* of T. Remember that T -> F -> H1 is a pipe. SHould we stratify by F? NO! That would block the pipe.
				- If you're looking for direct effect, remove it.
			- ^ Simplest example of "post-treatment bias." Researchers ruin their experiments during the analysis phase - not during the treatment phase.
				- Done by stratifying by a consequence of the treatment. Makes it seem as though treatment is unrelated to consequence, resembling more of the Fork. Or otherwise misleading.
				- Consequences of treatment should not usually be included in the estimator (imperfect rule of thumb)
				- Posttreatment bias isn't only caused by blocking mediators. There's a bunch of different ways that stratifying by post-treatment variables can happen.
					- Quick example: Treatment T, covariate T, unobserved compound u, outcome Y. ``T -> X, u -> X, Y``. Stratifying by X will make you think the treatment works.
					- We can explain this with the Collider.
	- 3) "The Collider"
		- ``X -> Z <- Y``
		- "The most upsetting elemental confound"
		- X and Y are not associated (share no causes) 
			- $Y \perp X$ - "Y is independent from X"
		- X and Y both influence Z, though.
		- Once stratified by Z, X and Y are associated.
			- $Y !\perp X | Z$ - "Y is not independent from X given Z"
			- Z is jointly caused by X and Y.
			- X and Y don't share any common causes! 
		- Toy example
			- Generate Bern. samples for X and Y, then a Z based on X and Y. 
		- Another example: grant proposals being approved
			- Grade proposals on a scale of trustworthiness and newsworthiness. 
			- We can see a clear negative correlation between trust. and news. in accepted grants, but this is caused by trustworthiness and newsworthiness in combination - one doesn't affect the other.
				- "Results in negative association, conditioned on award"
		- Colliders can also emerge in your estimator.
			- You can produce a spurious association between two unassociated variables
			- Example: age, marriage, and happiness
				- As time goes on, people are more likely to get married, so age affects marriage.
				- But let's say it has no effect on happiness. 
				- If we graph data points, it looks like as time goes on, there's a negative trend in happiness among married and unmarried people.
					- We just introduced a compound where there was non previously.
	- 4) "The Descendant"
		- ``X -> Z -> Y,A``
		- A is the descendant here. How it behaves depends on what it's attached to.
			- That is, when you stratify by a Descendant, it's like conditioning by the parent, but more weakly (not as direct.)
		- $Y !\perp X$ - X and Y are causally associated through Z
		- A holds information about Z
		- If strong enough, $Y \perp X | A$. Once stratified by A, X and Y become less associated. 
 		- Descendants are everywhere. They're often a proxy to what we want to really observe. Just need to be aware that it's often a diluted version of our target and exercise judgement.

## **06 - GOOD & BAD CONTROLS**
``https://www.youtube.com/watch?v=uanZZLlzKHw``

- General rule of thumb - try not to be clever. By that, we mean try to be reproducible and clear. 
	- Others can use same logic to challenge and improve on your work.
- Reminder - elemental confounds
	- Fork - ``X <- Z -> Y``
		- X and Y associated unless stratified by Z
		- Open by default. 
	- Pipe - ``X -> Z -> Y``
		- X and Y associated unless stratified by Z
	- Collider - ``X -> Z <- Y``
		- X and Y not associated unless stratified by Z
- We want some framework for combining elemental confounds and using them to draw conclusions about our estimator/estimand.
	- Ex. most basic causal inference problem - the confound.
		- ``X -> Y``, ``U -> X, Y``
		- Want to find causal influence of X on Y
	- What if we can't measure U to stratify by it? Then randomize X!
		- Effectively removes all arrows into X.
		- ``R -> X, X,U -> Y``
		- Remember - X is the treatment. 
		- AKA ``X -> Y, U -> X,Y`` without randomization, ``X -> Y, U -> Y`` with randomization (do(X))
	- We can't always do this. A lot of problems are observational, not experimental. 
		- Observational solution - stratify
		- Experimental solution - randomize
	- Also, sometimes experimental stuff is "monstrously unethical"
	- Also also, sometimes we can't fully randomize a variable. This would only remove influence to a partial degree.
		- Why? Noncompliance from human test subjects, or something like that
- Main idea - in an experiment, we **cut causes of the treatment.**
- Is there some statistical procedure that mimics randomization? How could this even work?
	- The goal is to, for any particular DAG, deduce some way to process the data to get a mathematical expression which is equal to the distribution of our outcome variable Y conditional on do(X)
		- ``P(Y|do(X)) = P(Y|?)``
		- ``P(Y|do(X))`` = "The distribution of Y given/stratified by our intervention on X"
	- We can do this mathematically. We can also do this just by looking at the graph.
	- Ex. simple confound from previous example
		- We know that to remove the confound, we stratify by U. But why does that work?
		- Because it's a fork, and to *close* the fork, we stratify by the center.
		- When we stratify by U, any remaining influence of X on Y is *not* because of U
		- $P(Y|do(X)) = \sum_(U) P(Y|X,U)P(U) = E_UP(Y|X,U)$
			- "The distribution of Y when we do X is equal to the distribution of Y given/stratified by X and the control variables (U), averaged over the distribution of the control variables"
				- Apparently there's an analytical system to do this. 
				- Read "The Book of Why" by Judea Pearl and Dana Mackenzie" and "Causality: Models, Reasoning, and Inference" by Judea Pearl"
				- Transform the DAG into an algebraic system and deduce the kinds of transformations/proper stratifications needed. 
			- Remember - the causal effect of X on Y is **not** the coefficient relating X to Y
	- Ex. cheetahs and baboons preying on gazelles
		- ``B -> G, C -> B, G``
		- Cheetahs eat a lot of gazelles and some amount of baboons. When cheetahs are present, baboons stay away from gazelles.
		- When cheetahs are absent, baboons eat even more gazelles than cheetahs do.
		- Therefore, the causal effect of baboons depends on the distribution of the cheetahs (!important point!)
	- For DAGs, rules for finding P(Y|do(X)) are known as do-calculus.
		- Says what is possible to say before picking functions
		- Justifies heuristic graphical analysis
	- Do-calculus is *worst case* - additional assumptions often allow stronger inference. 
		- We don't assign values to relationships (non-parametric), so see above.
	- Do-calculus is *best case* - if inference possible by do-calculus, does not depend on special assumptions.
- In this lecture, we'll focus on the **Backdoor Criterion**. A shortcut to applying (some) results of do-calculus.
	- Can be performed graphically/visually.
	- This is a rule to find a set of variables to stratify by to yield P(Y|do(X))
	- Steps! With example of ``X -> Y, U -> Z -> X, U -> Y``
		- 1) Identify all paths connecting the treatment (X) to the outcome
			- Ex. ``X - Z - U - Y`` and ``X - Y``
			- Causation goes one way, but association is both ways, so  direction doesn't matter for this.
		- 2) Paths with arrows entering X are backdoor paths. (confounding paths)
			- Ex. ``X - Z - U - Y`` is a confounding path.
		- 3) Find the "adjustment set" that closes/blocks all backdoor paths.
			- Ex. Block the pipe so $X \perp U | Z$ ("X is not associated with U given/stratified by an intevened Z")
			- Pipes and forks are **open** by default, meaning equal information can flow between the two. Stratify by the intermediate variables to close the patterns.
			- Colliders are **closed** by default. If you stratify by an intermediate variable, you will open it and may create a backdoor path.
			- Be careful when you stratify by certain variables. Don't want to accidentaly open a path.
			- Notify this stratification as ``P(Y|do(X)) = \sum_zP(Y|X,Z)P(Z=z)``
				- "The distribution of Y when stratified by X and Z averaged over the distribution of Z"
				- This tells us that our mathematical model to posterior distribution-ize is: 
					- $Y_i \sim Normal(\mu_i,\sigma)$
					- $\mu_i = \alpha + \beta_XX_i + \beta_ZZ_i$
				- Remember - here, linear regressions are useful for stratifying by certain variables. Use if appropriate!
			- Depending on the layout, there's some times when we can't estimate the exact direct effect, but we can estimate the total effect.
- Some (control) variables are good, some are bad.
	- What are some bad ones?
		- Anything in the spreadsheet
		- Any variables not highly collinear 
		- Any pre-treatment measurement.
	- See example of friends and hobbies affecting health. 
		- Being friends and health is unrelated, but when you add hobbies, you introduce a confound. Hobbies are tied to health and friendship.
- Let's talk colliders.
	- Don't condition on colliders! Already closed.
	- Sometimes, they aren't so obvious. Must consider the possibility of an unknown collider, sometimes.
- Case-control bias
	- ``X -> Y``, but also ``Y -> Z``
	- Don't select/stratify on an outcome or result of the outcome.
- "Precision parasite"
	- ``Z -> X -> Y``
	- Z isn't a backdoor, but still not good to condition on Z. 
	- Think like this - we're only after the direct effect of X on Y. Conditioning on Z works, sure, but it doesn't really actually matter and just adds noise.
	- "Bias amplification" - makes biased results worse. 

## **07 - FITTING OVER AND UNDER**
``https://www.youtube.com/watch?v=1VgYIsANQck``

- Generally, using a simpler model + estimator is desired.
	- In the real world, we're often trading off accuracy for simplicity
	- But why is accuracy related to simplicity anyways?
	- For any given situation, there could be multiple causal models that work.
		- Ergo for each model, an estimator exists.
			- Some are better and worse in the data.
	- Two main struggles when designing useful estimators
		- Struggle against causation
			- How to use causal assumptions to design estimators and contract alternative causal models with one another
		- Struggle against data
			- How do we make the estimator *work*? Make it effective? (Go figure.)
- Simple example: given data points of body mass to brain volume, predict a function.
	- When we make a line, consider how to make a function that describes these points.
		- (Fitting) How many parameters do we want to use?
			- Then, are there upsides/downsides to over/underfitting?
		- (Compression) IDK what this means yet
	- Also, which function explains these points?
		- Causal inference?
	- What if we intervene on a point's value?
	- Think about it like this:
		- We need a function that generates and need a function that fits to the output
	- Sometimes, your goal is to *predict* a value rather than gain insight into the causal model of some system
- Leave-one-out cross validation
	- Method of evaluating how good a line-fitting method is at predicting values.
		- Especially good with small dataset, but computationally expensive.
		- Once we have our estimator, how do we actually predict how good it is?
	- 1) Drop one point of the dataset.
	- 2) Fit the line to the remaining points.
	- 3) Predict the dropped point (based off x, predict y.)
	- 4) Go to step (1) with the next point.
	- 5) Score is the percent error on the dropped 
	- What does the equation for this look like?
		- $lppd_{CV} = \sum^N_{i=1}\frac{1}{S}\sum^S_{s=1}logPr(Y_i|\theta_{-i,s}) $
		- wtf does this mean?
			- $lppd_{CV}$ - "Log pointwise predictive density"
			- $\sum^N_{i=1}$ - Do this thing for each data point i of the N total data points.
			- $\frac{1}{S}\sum^S_{s=1}$ - Average of the below function on each s of the S samples from the posterior, 
			- $logPr(y_i|\theta_{-i,s})$ - The log distribution of 
			- Above two lines are the average log probability for point i
				- Typically, the predictive density is $p(y_i|y_{-i}) = \int p(y_i|\theta)p(\theta|y_{-i})d\theta$
					- Integral of posterior * probability of y_i given posterior parameters
				- Then, take the log of this and sum it. 
		- Ok, now that I understand it better.
			- $P(Y_i|\theta_{-i,s})$ - The likelihood of the omitted point $Y_i$ given the s-th posterior sample.
				- "Posterior sample" means a set of parameters taken from the post dist. For example, $\mu_i = \alpha + \beta * X_i$, where $\theta = {\alpha, \beta, \sigma}$ and $X_i$ is a predictor
			- $\frac{1}{S}\sum^S_{s=1}$ - The average log probability of the omitted variable arising from $\theta$ sampled from the posterior distribution.
				- Omit point i, form posterior distribution. Sample theta S times. For each sample s, find the probability of the omitted data point. Log this value and average all values per point.
			- $\sum^N_{i-1}$ - The sum of all averaged values calculated above.
				- Higher scores = worse, lower scores = better
			- In the video, it looks a little different. 
				- Calculate post. dist., sample theta S times is the same. But distance from line to omitted point seems to be measured, presumably averaged and summed for the score.
			- "Omit, dist., ", but the order is a bit unclear. And why is the score positive when the log of a probability is negative? And why does the video show distance from the curve to the omitted point?	 
	- We see that lower-order polynomials (i.e. linear models) are **worse in-sample** (aligning to data points within the training set) but **better out of sample** (predicting unseen data points.)
		- The inverse is true for simple models - more parameters improve fit to sample, but may reduce accuracy of predictions out of sample.
		- More accurate models offer flexibility alongside a tendency for *overfitting*.
	- Overfitting depends on priors.
		- **Regularization**: Function finds regular features of process.
			- Means "not being too exciteable" about every pointin the sample - not every feature in a sample is regular (doesn't represent the long run.)
		- We have a nice tool for dealing with this - priors.
			- Meaning we can tighten our priors to allow for less flexibility. 
			- Think about having a wide intercept prior distribution vs. a tight one that's scientifically justified.
	- Cross-validation by itself does nothing to produce good models (only comparative). Use regularization with priors to achieve good, skeptical models.
	- Ex. using wide vs. tight priors.
		- Tight priors help when predicting out of sample, but hurt when fitting in-sample.
			- Which makes sense. Can't reach outliers as well, but fit larger dataset more appropriately.
			- You *can* make priors too tight. At some point, they start getting worse both in-sample and out of sample. "Extremely skeptical" due to small sample + prior sizes.
	- How do we choose priors?
		- For causal inference, use science.
		- For pure prediction, can tune the prior using cross-validation.
		- Most tasks are a mix of inference and prediction.
		- No need to be perfect - "just better."
- Prediction penalty
	- For polynomial counts, find difference of prediction errors between in-sample and out-of-sample. This is the *prediction penalty.* Try graphing it.
	- Goes up and up - would be useful to see this behavior of a posterior distribution without having to refit N models each time.
		- We can!
			- "Importance sampling" (PSIS)
			- "Information criteria" (WAIC)
	- WAIC, PSIS, CV measure overfitting through the prediction penalty term.
	- Regularization manages overfitting.
	- But none of these directly address causal inference.
		- They're important for understanding statistical inference, but they're only pieces to the puzzle.
		- DONT use predictive criteria (WAIC, PSIS, CV) to choose a causal estimate.
			- They actually prefer confounds and colliders. See plant growth experiment.
- Outliers and robust regression
	- Some points are more influential than others
	- Outliers indicate predictions are possibly overconfident, unreliable.
		- The model doesn't expect enough variation, which makes sense.
		- So, dropping outliers doesn't actually fix anything. Just ignores the problem - predictions are still bad!
			- The model's wrong, not the data.
			- So what do?
				- Quantify the influence of each point.
				- use a mixture model (robust regression)
## **08 - MARKOV CHAIN MONTE CARLO**
``https://www.youtube.com/watch?v=rZk2FqX2XnY``

- Funny story - can approximate the digits of pi by randomly throwing sausages on the ground.
	- Just an example of using random numbers to accomplish computation.
- Moving on, we'll update our post. dists. using (P)RNGs
- Take the previous marriage problem, for example
	- We haven't actually observed any of the variables in the sample - just descendants. 
- Or an example of a classroom
	- Our challenge here is latent modeling problems.
	- Let's say there's some level of knowledge K you want to assess in a student. 
		- Obviously we can't directly measure this.
		- But we can indirectly measure it by giving tests, asking questions, etc.
			- Knowledge K affects Test Score S. But S is an imperfect proxy of K - not 100% correlated.
		- Also, there may be difficulty or imperfection of the test/instrument. Call this Q.
		- Also also, there may be some feature X of the person. Not necessarily their knowledge, but affects scores and knowledge.
- Take another example - academic papers
	- Feature X has some impact on unobservable quality Q, both of which impact evaluation Y. Uobserved referee R also impacts Y.
- So how do we deal with unobserved factors? How do we calculate the posterior distributions?
	- Real research problems deal with the following problems a lot.
		- Many unknowns/parameters
		- Nested relationships
		- Bounded outcomes
		- Difficult calculations
- "Analyze the data" (compute+sample the posterior) - how do we do this in more complicated scenarios, exactly?
	- Analytical approach (often impossible)
		- Count paths through the garden of forking data
		- Relies on being able to integrate the (simple) function
	- Grid approximation (very intensive)
	- Quadratic approximation (limited)
		- AKA Laplace Approximation 
		- This is quap
		- Limited because it relies on gaussian distributions
	- MCMC (intensive)
		- Workhorse of modern statistics
		- Also intensive, but more flexible than quap
		- Hear ye, hear ye! The parable of King Markov
			- King Markov's obligation is to visit the islands of his people
			- His kingdom is the Metropolis Archipelago, made up of islands. Let's say 7 islands.
			- He will spend time on the islands in proportion to their population sizes
			- But King Markov doesn't like to keep records and whatnot. Wants a simple, *day-to-day* rule of thumb to follow.
			- Here are the steps to the algorithm
				- 1) Flip a coin to choose island on the left or right with 50% probability. Call it the "proposal island" - where the coin proposes for King Markov to go.
					- In the example, King Markov starts at island 4, $p_4$. His proposal is the island to the right, island $p_5$ 
				- 2) Find population of proposal island.
					- In this case, population of $p_5$
				- 3) Find population of current island
					- In this case, population of $p_4$
				- 4) Move to proposal, with probability $frac{p_5}{p_4}$
					- Either the king moves to the proposal, or he stays at the current island.
					- If the proposal island has more population, he's guaranteed to go.
						- Otherwise, he has a smaller (but doable) chance to go  
					- Initial 50/50 makes it possible but highly unlikely to stay in sparse islands
					- Proposal / Current
		- Obviously, this story doesn't exist.
		- But it's an example of an MCMC algorithm called the Metropolis algorithm.
			- Typical Metropolis use: draw samples from a posterior distribution
				- Used to do calculus without actually doing calculus
			- "Islands": parameter values
			- "Population size": Posterior probability
			- Visit each parameter value in proportion to its posterior probability
			- Any number of (parameter) dimensions
		- "Markov chain Monte Carlo" - ??? wtf ???
			- Chain = Sequence of draws from the distribution
			- Markov chain = History doesn't matter - just where you are now
			- Monte Carlo = random simulation
		- Metropolis alg. is a simple version of MCMC
			- Easy to write, often inefficient
	- Rosenbluth actually did a lot of work which led to Metropolis algorithm
	- Nowadays, we use more efficient algs. inspired by Metropolis
		- The best methods use gradients.
			- We'll explore one such alg. - Hamiltonian Monte Carlo
	- To introduce Hamiltonian MC, let's first look at Metropolis visually
		- Parameters on each axis, forms a 2D post. dist. like before.
			- If you need to turn something into a smooth space, take the log of it.
		- Start at a random point, point in a random direction, choose random distance (forming a vector,) propose, move (or not)
		- We can't do the integral calculus on the posterior distribution, but we can still sample from it. 
		- We can tune Metropolis further
			- Adjust step size to be large/small
				- Large means traverse the space faster, but will have more rejected proposals
				- Small means traverse the space slower, but will have fewer rejected proposals
				- "Traversing the space" means holistically sampling from the *entire* post. dist., not just the likely areas.
	- Then, let's discuss gradients
		- When you take the log of a Gaussian distribution, it forms a "Gaussian bucket" of sorts
			- Works in 2 dimensions, 3 dimensions, etc.
		- Think about a skate park - 2d Gaussian bowl
			- The bottom of the bowl is the high probability space. Skater will spend the most time here
			- The board doesn't need to know anything about the bowl, but it'll end up here most of the time just by following the slope of the bowl.
	- Finally, let's discuss Hamiltonian Monte Carlo
		- You have your post. dist.
		- Choose a spot, choose direction and magnitude
		- Start to "roll the marble into the bowl" for some step length
		- Based on the current magnitude and direction, keep moving and updating based on the Gaussian gradient
		- Works in n-dimensions - just can't visualize it 
		- Like Metropolis, we can adjust parameters
			- High step size -  marble loops around a bunch of times in the bowl before reporting position
				- Inefficient - we often end up near the same spot just by chance. Called the "U-turn phenomenon." Don't want this
			- Low step size - marble moves more varied, but again, slower to explore entire curve
	- Take the example of two highly correlated variables
		- This is fine. We're expecting to spend most of our time in the bottom of the bowl.
		- Metropolis would try to make a bunch of random guesses and have them mostly get rejected, since low probability of going anywhere except for bottom of bowl
			- Hamiltonian keeps us in the middle, following the curvature of the post. dist.
	- HMC requires gradients. How to procure?
		- We can provide them ourselves
		- We can have Stan do it for us with Auto-diff
			- Each function can be broken down into smaller functions with the chain rule, then derived from there.
			- Consider symbolic derivatives of your model code
			- used in many ML approaches. "Backpropogation" is a special case.
- Example: the Judgement at Princeton
	- Wine tasting judgement in New Jersey
	- Made up of: 
		- 20 wines (10 French, 10 NJ, all blinded)
		- 9 French and American judges. 
		- 180 scores
	- This is a way for us to introduce "item response"
	- For each score, consider the following variables:
		- Wine quality Q
			- This is what the whole competition is really about. 
			- We can't observe true quality, though
		- Score S
			- Each wine has a score. 
			- We **can** observe this.
		- Judge J
			- Who gave the score in the first place?
			- They have some preferences or other unobservable characteristics/disposition, affecting score.
		- Wine origin X
			- Where does the wine come from?
			- Affects Q directly
			- Affects S directly **and indirectly**
				- If a French judge knows they're drinking a French wine, they will probably rate it higher.
			- Observed
		- Origin of judge Z
			- To address bias, we can observe the judge's origin
			- Affects J (individual and their predispositions)
	- Relationships:
		- ``X -> <Q> -> S``, ``Z -> <J> -> S``
			- We try to avoid X influencing S by blinding the wine, removing ``X -> S``
		- No confounds, but our estimand is estimating wine quality and stratifying it by origin
			- Not needed for confound purposes, but eliminates any potential unobserved confounds anyways
				- Maybe the blind isn't perfect, in which case there's a fork that requires statifying by X anyways.
	- Our estimand is the association between the quality of wine and its origin
		- Stratify by X for potential confounds
		- Stratify by J for efficiency
			- Not a confound, but competing cause for the score.
	- Process:
		- Prepare mathematical model
			- $S_i \sim Normal(\mu_i,\sigma)$
				- Scores are standardized
			- $\mu_i = Q_{W[i]}$
			- $Q_j \sim Normal(0,1)$
				- Latent, arbitrary variable. We can assign it whatever.
			- $\sigma \sim Exponential(1)$
		- Prepare data in R
			- Standardize S
			- J is an index for a specific judge
			- W is an index for a specific judge
			- X is an index for wine origin. 1=America, 2=France
			- Z is an index for judge origin. 1=America, 2=France
			- Later on, we add a couple new parameters. No reason not to do this!
				- H - Judge harshness
					- Subtracted from quality/origin offsets
					- How good the wine needs to be for the judge to rate it as average
				- D - Judge discrimination
					- Coefficient of all previous operations
					- How much little differences matter to the judge
				- Like bias and 
		- Then, estimate relationships!
			- Run the model
			- Do some sampling, build the Markov chain
			- Autodiff, find gradeints
			- Run Hamiltonian simulation
			- Display output
	- Markov Chains are complex, but have a lot of diagnostic options
		- 1) Trace plots
			- Visualization of Markov chain
			- Look for fuzzy caterpillar when chains are overlaid.
				- What does this mean? Look for it to explore the parameter space consistently.
				- Good chains overlap with one another well
				- Bad chains have lots of gaps, don't explore the space well. 
					- Inefficient. The chain will eventually explore the post. dist., but it's gonna take a ton of samples and some prayer
			- Gray region at the start = warmup region (not samples from the post. dist.)
			- Need to fit more than 1 chain to assess "convergence"
				- This is how we get the overlaid chains - by fitting multiple.
				- "Convergence" - Each chain explores the right distribution and every chain explores the same distribution 
		- 2) Trace rank (trank) plots
			- Same data as a trace plot, but take rank orders of samples and plot them.
			- Makes geometric shaped chain overlay. 
			- You want to see that all chains generally share the top/bottom equally, frequently intermixed. Still uniform, like the fuzzy caterpillar.
		- 3) R-hat convergence measure
			- Remember our convergence criteria
			- R-hat is a ratio of variances
			- Given that our chains should be exploring the same space, the average variance should converge on 1
			- Not a test! No guarantees
		- 4) Number of effective samples
			- AKA n_eff
			- Approximation of how long the chain would be if it were perfectly uncorrelated.
			- Alternatively, when samples are *autocorrelated*, you have fewer effective samples.
			- Typically, will be shorter than your number of samples. Bigger number = better.
				- Sometimes, Stan will be so good that it'll do better than perfectly random. Ut's not a bug if you see larger than your sample size.
		- 5) Divergent transitions
			- We'll talk about them at a future time.
			- A type of rejected proposal

## **09 - MODELING EVENTS**
``https://www.youtube.com/watch?v=Zi6N3GLUJmw``

- Last week, we introduced MCMC and HMC
	- Another difficult machine to handle.
- Remember - the theme of this course is to flow.
	- But you don't have to understand everything instantly to keep moving forward
- Modeling events is tricky
	- Discrete, unordered outcomes
		- Like the globe-tossing sim at the start
	- Observations are counts
	- Uknowns are probabilities, odds
	- Everything interacts always everywhere
		- Can't treat everything as independent or siloed
	- Using log odds from now on
- Remember our typical process - estimand, scientific modeling, statistical models, analyze and interpret
- Our introduction to events is PhD admissions for UC Berkeley in the 70s/80s
	- Stratified by: 
		- Department (Physics, psych, etc.)
		- Gender of applicant
	- Estimand: was there gender discrimination in graduate admissions?
		- ``G -> A``
		- Departments have varying availability.
			- ``D -> A``
		- Different genders typically populate certain fields/departments more frequently
			- ``G -> D``
		- When we refer to gender, what's the actual impact? Probably the perceived gender.
			- i.e. If an officer sees their gender, do they act differently?
			- ``G -> G* -> A``
			- ``R -> A``
			- We can try to blind ``G* -> A``
		- Which of these paths is "discrimination?
			- ``G -> A`` is direct discrimination - when a referee sees a gender, they behave for/against them
			- ``G -> D -> A`` is indirect discrimination - different genders are disproportionately affected by departments
			- Both make up total discrimination - what a person experiences
				- Requires mild assumptions, where the others require strong assumptions
			- Often, what we can estimate is not what we want.
			- Ignore confounds for now
		- When we do a simple model with no direct discrimination, the gender preferences for departments make the acceptance rates uneven.
			- But when we drop the rates for gender 1, the overall pattern is the same.
			- Note: this is a really unsatisfying model
		- Remember - **observe** counts of events, **estimate** probability (log-odds) of events
			- Ex. globe tossing model - we need the proportion of water stratified by other variables
		- Good time to introduce **generalized linear models**
			- Linear models: expected value is additive ("linear") combination of parameters
				- $Y_i \sim Normal(\mu_i, \sigma)$
				- $\mu_i = \alpha + \beta_XX_i + \beta_ZZ_i$
				- Generally, you can only do this with a normal distribution because it isn't bounded on either side.
					- Probabilities need to be restricted between 0 and 1. Linear models break this.
					- We can address this. Use generalized linear model to turn linear model into a line that fits. 
						- GLM is a function of probability that outputs a linear model. 
			- Generalized linear model: expected value is some function of an additive combination of parameters
				- $Y_i \sim Bernoulli(p_i)$
				- $f(p_i) = \alpha + \beta_XX_i + \beta_ZZ_i$
				- Let's explain this with the Berkeley example
					- $Y_i$ 0/1 of some event happening. Powered by a probability - in this case, $p_i$
					- $f$ is the **link function** because it links the parameters of distribution to a linear model.
					- $f^{-1}$ is the **inverse** link function. Converts linear model to probability 
			- Distributions are a relative number of ways to observe data given assumptions about rates, probabilities, slopes, etc.
				- Distributions are matched to constraints on observed variables
					- Probability distributions are just ways of counting up number of ways data could arise given assumptions
					- When we do this, the probability distribution generally matches a small family of common distributions
				- Link functions are matched to distributions of outcome variable
- Exponential probability distributions
	- Family of common distributions, start by building up from the Exponential distribution.
		- Exponential distribution - $y \sim Exponential(\lambda)$
			- Distribution of the time to an event that has a constant rate in time
			- Ex. waiting for the bus.
				- t represents time passing, p is probability that you're still waiting for the bus
					- Start out at 1. You **must** wait some amount of time.
					- As time goes on, the probability of you still waiting goes down.
						- Probability goes down because as time goes on, the event has had more opportunities to happen.
			- Values sampled from the posterior distribution are **latency** (time). Greater than 0, continuous values.
		- Binomial distribution - $y \sim Binomial(n,p)$
			- When we count the events associated with an exponential process, we get the Bionmial distribution.
			- Looks kind of like the normal distribution. Exponentially distributed. Bounded in time.
			- If you have a process producing events in continuous events and you count those events, you get a binomial distribution.
				- Set an observation window, observe exponential processes.
			- Ex. Fish swimming along a river
				- Swimming at an exponential rate
				- Constant time
				- Will observe them exponentially distributed.
				- If you do so between some time frame (like noon to 2:00) and count the number of fish, they will have a roughly binomial distribution
		- Poisson distribution - $y \sim Poisson(\lambda)$
			- Special case of the Binomial distribution where we don't know the maximum.
				- Ex. we don't know the number of fish
			- Fundamentally the same as the count of the exponential distribution - this is why it shares the $\lambda$ parameter.
		- Gamma distribution - $y \sim Gamma(\lambda, k)$
			- Sum up exponential processes
			- Generally arises when multiple exponential processes are summed together
		- Normal distribution - $y \sim Normal(\mu, \sigma)$
			- Absorbing state - "can't get out once you're there."
			- Large mean of Gamma
		- **KEY TAKEAWAY** - know the constraints and observations. 
			- You cannot "test" if your data is "normal"
			- Distributional assumptions are assumptions about constraints on observations.
				- i.e. Distributions are matched to **constraints** on observed variables
				- **Link functions are matched to distributions.**
- Logit link
	- Logit = log odds. Read as such.
	- Bernoulli/Binomial models use this frequently
		- Remember what these actually are
			- Bernoulli is a coin toss distribution
			- Binomial is a sum of coin tosses
	- $logit(p_i) = log \frac{p_i}{1 - p_i}$
	- This lets us attach linear models to odds
	- We think of probabilities as they are, but all the machinery uses log-odds scale
		- "Log-odds scale": the value of the linear model
		
