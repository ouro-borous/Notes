bayesian_statistics
===================
- Category: Learning
- Tags: 
- Created: 2024-12-20T18:05:42-08:00

## **01 - GOLEM OF PRAGUE**

``https://www.youtube.com/watch?v=FdnMWdICdRs``
- Thinking about how science influences statistics (an applied form of mathematics)
- Bayesian inference vs. frequentism is a common debate in stats courses
	- Won't discuss it though. Looking at causal inference instead. Bayesian debate is mostly settled
		- Science before statistics.
		- For statistical models to produce scientific insight, they require additional scientific (causal) models where changing a var changes another
		- The reasons for a statistical analysis are not found in the data but in the causes of the data. Numbers mean nothing without context.
			- Data cannot be extracted from causes alone (requires testing.) Likewise, causes cannot be extracted from data alone. No causes in; no causes out.
		- What is causal inference?
			- More of an association between variables
			- "Causal inference" is prediction of intervention
				- Knowing a cause means being able to predict the consequence s of an intervention by that cause.	
				- Ex. tree swaying outside in the wind is caused by wind. The two are associated and one can cause the other, but we're usually sure it's the wind causing the swaying.
			- "Causal imputation" is imputation of missing observations
				- What does this mean? Knowing a cause lets you ask "what if xyz was done differently?" (construct unobserved counterfactual outcomes)
			- Causal inference is related to description (of population) and design (of project)
				- These all rely on some scientific model
- DAGs (Directic Acyclic Graphs)
	- Essentially highly abstracted causal models - the only info in a DAG are variables and one-way causal relationships.
	- All a DAG defines is relationships - no more, no less. Used to clarify thinking.
	- Analyze this to produce appropriate statistical models.
	- "What can we decide, without additional assumptions?"
	- See video example
		- X -> Y
		- C -> X, Y
		- A -> C, X
		- B -> C, Y
		- X influences Y, which is what we want to test. Other variables influence these, though.
			- X and B are competing causes of Y
			- A is an influence of the treatment X
			- C is a common cause of X and Y - a compound to control for
	- DAGs let us ask questions and consider different models
		- Which control variables? DONT ADD EVERYTHING
		- How do we test and refine the causal model?
	- Also, they let us focus more on science rather than data
- Golems
	- The course's name for statistical models
	- Named after golem of Judaism. It's a hollow puppet that performs a purpose regardless of intent.
		- Likewise, statistical models have no inherent purpose.
		- They are also powerful, intent-agnostic, and dangerous
	- Quick rant about frequentist statistical models
		- Incredibly limiting
		- Focus on rejecting null hypothesis
		- Relationship between research and test unclear
		- Industrial framework
		- "Follow the flowchart" approach not ideal
	- Lots of studies revolve around complex systems - null models are rarely not feasible
		- Null pop. dynamics?
		- Null phylogeny?
		- Null social network?
		- Problem: many processes produce similar distributions
	- Example: how does DNA evolve? Neutral evolution vs. selection?
		- The two hypotheses lead to distinct process models, but overlapping statistical models
	- We're going to start with DAGs, turn them into generative causal models, then write statistical models to analyze synthetic data, then introduce real data.
		- Start with abstracted view
		- Generate synthetic data based on our abstracted DAG view
		- Write statistical models to analyze synthetic data
		- Use statistical models on real data
			- Like training an ML model!
	- General process:
		- Make a DAG, select "adjustment set" (relevant variables)
		- Need generative model of causal model to design/debug our code
		- Need a strategy to derive estimate and uncertainty from finite data
			- Easiest approach: bayesian data analysis. It's easiest to use because Bayesian models are generative - simulate the data yourself
- Owls
	- Common joke - step 1 is draw circles, step 2 is draw the rest of the owl
		- But sometimes it feels like this when teaching computational materials
	- We're interested in documenting our workflow.
		- Understand what you're doing
		- Reduce error, allow others to analyze your work better and find problems.
		- Respectable scientific workflow
	- 1) Theoretical estimand - what are we even doing in the study?
	- 2) Scientific (causal) model(s) - start as a DAG, but needs to end up being generative.
	- 3) Use (1) and (2) to build statistical models
	- 4) Simulate from (2) to valdiate that (3) yields (1)
	- 5) Analyze real data

## **02 - GARDEN OF FORKING DATA**

``https://www.youtube.com/watch?v=R1vcdhPBlXA``
- Imagine you're trying to model the proportion of land to water on the surface of the earth.
	- We can estimate this by using a model globe and tossing it, then seeing which type of surface (land/water) lands upright. Eventually, this should converge on the proportion.
- Here's our general workflow.
- 1) Define generative model of the sample
	- Start with DAG style - how do variables work with one another?
		- Which variables are we even working with?
			- p - proportion of water (our goal to figure this out)
			- n - number of tosses
			- l - number of land observations
			- w - number of water observations
	- n influences W and L - the more tosses you do, the values increase/decrease.
	- p influences W and L - as the proportion changes, the number of W/L changes.
		- Intervening on one of these values will influence the values of W and L
	- To make this generative, we need to define some function. W,L = f(p, n)
		- This can be really any function you want, but it needs to reflect the scientific scenario.
- 2) Define a specific estimand
	- The globe being tossed is our estimand
- 3) Design a statistical way to produce estimate
	- Bayesian data analysis - simple, humble way to do this.
	- "For each possible explanation of the sample (proportion of water), count all the ways the sample could happen."
	- "Explanations with more ways to produce the sample are more plausible."
	- Imagine a garden of forking paths. There's many datasets that can arise - think about all the different ways things can happen and certain decision points.
		- Ex. doing infinite datasets of the globe's infinite proportion in infinite tosses would be tough. Instead, let's do a 4 sided die and toss it 3 times for WLW.
		- For a D4 globe with fully covered sides, there's 5 possible globes (representing a abstracted subset of the real globe's proportion.)
			- LLLL
			- WLLL
			- WWLL
			- WWWL
			- WWWW
		- For each example proportion of W:L, find how many ways our given sample could arise.
			- Globe	 W | L | W
			- LLLL - 0 * 4 * 0 = 0 ways (there were 2 W in sample)
			- WLLL - 1 * 3 * 1 = 3 ways
			- WWLL - 2 * 2 * 2 = 8 ways
			- WWWL - 3 * 1 * 3 = 9 ways
			- WWWW - 4 * 0 * 4 = 0 ways (there was 1 L in sample)
				- See Forking Path diagram in video - useful to visualize
		- We can see that WWWL has more ways to produce - but the sample isn't very large, so there isn't much support to claim that the glopbe is WWWL.
			- This is a good feature of an estimator - not overconfident in small samples
		- We just produced a Bayesian analysis!
		- Now, let's do some Bayesian updating. Increase the size of the sample (roll again), so we now work with WLWW.
			- We can just multiply our values by the number of ways that the new sample can happen.
			- Globe	 WLW | W
			- LLLL - 0   * 0 = 0 ways
			- WLLL - 3   * 1 = 3 ways
			- WWLL - 8   * 2 = 16 ways
			- WWWL - 9   * 3 = 27 ways
			- WWWW - 0   * 4 = 0 ways
		- We can see the values starting to seperate a bit.
		- What about larger samples, like WLWWWLWLW? How can we express this mathematically?
			- Globe	 W | L | W | W | W | L | W | L | W
			- LLLL - 0 * 4 * 0 * 0 * 0 * 4 * 0 * 4 * 0 = 0^6 * 4^3 = 0 ways
			- WLLL - 1 * 3 * 1 * 1 * 1 * 3 * 1 * 3 * 1 = 1^6 * 3^3 = 27 ways
			- WWLL - 2 * 2 * 2 * 2 * 2 * 2 * 2 * 2 * 2 = 2^6 * 2^3 = 512 ways
			- WWWL - 3 * 1 * 3 * 3 * 3 * 1 * 3 * 1 * 3 = 3^6 * 1^3 = 729 ways 
			- WWWW - 4 * 0 * 4 * 4 * 4 * 0 * 4 * 0 * 4 = 4^6 * 0^3 = 0 ways
		- So our equation is W,L = (4p)^w * (4-4p)^l
		- As our samples start to get larger, the numbers of ways that things can happen will also increase exponentially. Better to convert it to probability.
			- How do we do this? Simple. Add all the possibilities up, then divide each possibility value by the total to get the percentage.
			- This collection of (proportion) probabilities is called the "posterier distribution"
- 4) Test (3) using (1)
	- At this point, we have designed an optimal Bayesian estimator (if the model is correct.)
		- What does this mean? So long as our generative model is correct/representative of real data, our estimates for likelihoods of p are optimal given generated data.
	- We want to test before we estimate and summarize.
		- 1) Code a generative simulation
		- 2) Code an estimator
		- 3) Test (2) with (1)
	- We can do 4-sided globe, 10-sided, 20-sided, etc.
		- But we can't do infinite-sided. This is the limitation. So how do we get our maximum?
		- Let's think about it logically.
		- The globe is a polyhedron with infinite sides
		- The posterior probability of any "side" p being water is proportional to:
			- (p^w)((1-p)^l) 
		- Now, all we need to do is normalize this and we get our Beta distribution - a single continuous function, instead of lines.
			- Beta distribution made up of of normalizing constant * relative number of ways to observe sample.
				- In this case, relative number of ways to observe sample is the function we calculated earlier - (p^w)((1-p)^l)
				- We then normalize this value to turn it into a probability in correspondence to the total number of possibilities.
					- Shape of the curve doesn't depend upon the normalizer at all. That's all chosen by our original function - normalizer just normalizes.
				- Beta distribution normalizes to density - 0.0 upward
				- Forms a curve when graphed. This curve starts to narrow and become higher as more samples as we update
	- A few lessons
		- 1) There's no minimum sample size. This is good, but unlike frequentist probability practices. Bayesian estimators work with small samples - it's just gonna be a pretty flat curve.
		- 2) Shape embodies sample size. This is really nice - we update the shape of the post. dist. when updating the samples. Also, see above.
		- 3) No point estimate. Use the whole distribution every time. Don't use mean/mode/whatever - the whole distribution, or a subset range of it, is relevant. THE DISTRIBUTION IS THE ESTIMATE.
		- 4) No one true interval. Intervals communicate shape of posterior, but the choice is pretty much arbitrary.
- 5) Analyze real sample, summarize
	- Now that we have our posterior distribution (a type of estimate), now what?
	- If you can do a single calculation on one point (proportion in this case), you can and must do it to every point as you randomly sample to create a new doistribution.
		- But to do this, we need to sample from the posterior distribution.
			- Usually, this requires integral calculus. Thankfully, we can approximate by sampling from the beta distribution. 
	- Video example: sample from posterior distribution. Then simulate the sampling again. Then plot. This makes a curve that accounts for uncertainty.
	- Sampling is fun and easy!
		- Sample from posterior, compute desired quantity for each sample, profit. Much easier than doing integrals.
		- Turn calculus problem into a data summary problem!
		- Markov Chain Monte Carlo (MCMC) produces only samples, anyways
- Notes on "Bayesian Modesty"
	- No guarantees except logical
	- Probability theory is a method of logically deducing implications of data under assumptions of your choice.

## **03 - GEOCENTRIC MODELS**

``https://www.youtube.com/watch?v=tNOu-SEacNU``
- Last time, we introduced basic logic of Bayesian inference
- This time, we're talking about expanding our Bayesian logic
- Example given - to explain Mars apparently "wandering", the geocentric model of the solar system was introduced
	- This worked well to predict the way that Mars would appear, but didn't really capture the mechanics of the solar system
	- Gauss took a stab at making a solar model and came up with normal error and least-squares estimation
		- We typically call his error distribution the "Gaussian distribution"
- This brings us to the topic of linear regressions
	- Like geocentric model, they describe associations and make predictions, but mechanistically wrong.
		- The universe is not comprised of perfect linear relationships. 
 		- The geocentric model is not inherently useless - it's just not a good generative model to represent the solar system. Good for planetariums, tho.
			- Linear regressions are powerful golems, but need appropriate usage
	- Lin. reg. is Gaussian by nature. We will use the Gaussian error model. 
		- Gauss defined very general conditions for how error creeps into observations
		- Gaussian distribution - there's a bunch of little errors that get added together
		- Ex. football field with lots of people on a line, heads = move right, tails = move left 
			- Over time, this will resemble a Gaussian distribution.
			- Why? Why do natural forces attract Gaussian (normal) distributions?
				- Fluctiations generally add and result to the more possible outcome.
				- Ex. more ways for people to end up in the middle than there are for people to end up far away.
- Two main arguments for why Gaussian (normal) distributions emerge
	- Generative - summed fluctuations tend towards normal distribution
	- Inferential - for estimating mean and variance, Gaussian distribution is least informative.
		- Often the best - all it does is give us a mean and variance. No more, no less.
		- A variable doesn't need to be normally distributed for normal model to be useful. It's a machine for estimating mean/variance.
- Remember our workflow. Let's update it.
	- 1) State a clear question/goal/estimand
	- 2) Sketch causal assumptions. Scientific model.
	- 3) Use the sketch (2) to define a generative model
	- 4) Use generative model (3) to build an estimator
	- 5) Validate model. Go back to (2) and start from there again if needed.
	- 6) Analyze real data.
- Ex. height vs. weight measurements
	- 1) State a clear question/goal/estimand
		- Describe association between weight and height
		- Make sure to state assumptions. We're only looking at adult weight/height 
	- 2) Sketch causal assumptions. Scientific model.
		- H -> W
			- Can also write this as W = f(H)
			- "Weight is some function of height"
			- We can quickly speculate about our generative model
				- Dynamic - incremental growth of organism, both H and W derive from growth pattern, Guassian variation result of summed fluctutations
				- Static - changes in height = changes in weight, but no mechanism. Gaussian variation result of growth history
		- But what about uncertainty? H -> W, U -> W
			- W = f(H, U)
	- 3) Use the sketch (2) to define a generative model
		- Start simple - W = Beta * H + U
			- "Weight is some proporion of H plus some uncertain factor"	
		- Let's also introduce some good conventions. When describing models...
			- List the variables 
			- Define each variable as a determninistic or distributional function of the other variables
		- So let's describe our variables now. i is the index for a single data point, = is "deterministic" and ~ is "distributed as"
			- Wi = Beta * Hi + Ui
			- Hi ~ Normal(0, sigma)
			- Ui ~ Uniform (130, 170)
			- Describe in this order with words, but for code, you need to describe the small components before your general statement.
	- 4) Use generative model (3) to build an estimator
		- When we build our stat. model, we're going to use some stuff that wasn't used in the generative model, since they don't typically follow the same shape. 	
			- Remember, this is a linear regression. The process is like last time with the globe, only now, we're converging on the parameters for a lin. reg. instead of a proportion of globe.
			- We'll make our statement for Bayesian updating our priors. For now, let's first define the statistical model.
				- E(Wi|Hi) = alpha + beta * Hi
					- "Our estimate for Wi given Hi is intercept alpha plus slope beta times Hi
					- Wi = average weight conditional on height
			- So what's our posterior? Remember, we're trying to find distributions for a specific line. 
				- Alpha and beta define the line for a specific data point (expected weight for each height)
				- Pr(alpha, beta, sigma | Hi, Wi) = (Pr(Wi | Hi, alpha, beta, sigma) * Pr(alpha, beta, sigma))/Z
					- Break this down intosmaller parts. What does this yucky statement even mean?
					- Pr(alpha, beta, sigma | Hi, Wi) -> We are trying to find a multivariable posterior distribution.
						- Post. dist. = the only estimator in Bayesian inference
						- In the past, we estimated the posterior distribution of the proportion of water to land.
							- This took our previous distribution and multiplied each proportion by the relative number of ways to achieve the new sample.
							- With WLW and a new W observation, It was (WLW possibilities) * (new possibilities to get W per each proportion
						- Now, we're taking multiple previous distributions and multiplying them by the new number of ways to achieve our 
		- Let's talk about priors real quick.
			- When doing the globe example, we started with 2 W and 1 L. We know that there's water and land on the globe, so our starting sample reflects reality already.
			- What about when we start from scratch? We need some value to start at - or rather, a distribution. Up to you to choose.
- At this point, I must be honest - I'm confusing myself a bit. So let's go from the beginning and take it slowly.
	- 1) State a clear question/goal/estimand
		- We want to describe the association between weight and height.
		- We're only looking at adult weight and height, so filter for any data points over the age of 18.
	- 2) Sketch causal assumptions. Scientific model.
		- In our globe example, there was one unobserved variable - the proportion of water to surface on the globe. This influenced the number of W/L observations. 
			- Now, we've got height influencing weight, but we've also got some other factors that we don't know about.
			- We'll call height H and the unclear stuff U. Our predictor function is loosely defined as ``W = f(H, U)`` 
	- 3) Use the sketch (2) to define a generative model
		- Looking at the data, we can also kind of see a linear relationship, so we'll model a linear regression.
			- The real equation probably looks something like W = beta * H + U
			- Let's also define what our variables will look like. Some variables are "random" (from a given distribution), some variables are deterministic functions of others 
				- ``Wi = B*Hi + Ui`` -> W is a function of other variables. Wi = beta(slope) * Hi + Ui
				- ``Ui ~ Normal(0,sigma)`` -> Ui is a normal distribution. **at least when we're generating.**
				- ``Hi ~ Uniform(130, 170)`` -> Hi is a uniform distribution from lower height to higher height
				- Beta is the slope that we're trying to estimate. Hi is observed. We don't give beta a value until we set priors.
	- 4) Use generative model (3) to build an estimator
		- Our estimator is going to resemble our previous generator. This time, however, we're trying to find certain unobserved values. Beta and Ui (alpha) were unobserved, 
			- ``E(Wi | Hi) = alpha + beta * Hi`` -> Read as "our estimate of Wi given Hi follows the following function:"
				- **KEY IDEA:** Our estimator is the final product. We're trying to complete the estimator using posterior distributions. 
		- Now we need to build our posterior distribution.
			- A couple things are different this time, though
				- We're doing a multivariable posterior distribution. It's still one distribution, but you can think of it as multiple one-variable distributions. Kind of.
				- Take a look at the equation in the video at ``38:30``
					- The left hand side is the posterior probability of a given line (as in the aforementioned multivariable distribution)
					- The left hand numerator on the right hand side is our "garden of forking data" - number of ways to get our 
					- The right hand numerator on the right hand side is our prior - previous posterior distribution
					- Z is our normalizing constant. Don't worry about it for now. In fact, you can kind of think of the numerator alone as the "number of ways the line could happen" like previously
						- Posterior = prior * number of new ways for sample to arise / normalizing constant
				- Normally, we don't write it like this. We can predict that Wi will converge on Gaussian
					- We write it as Wi ~ Normal(mu sub i, sigma), where mu sub i = alpha + beta * Hi. **This is our statistical model** that we're trying to posterior distribution-ify.
		- Choosing priors
			- We need to start with something when we have no priors. So we'll choose distributions based on what's scientifically justifiable. Up to preference.
				- Choose your priors *softly*. Weight generally goes up with height, when W=0 H=0, etc.
			- Think about our statistical model. Wi ~ Normal(mu sub i, sigma), where mu sub i = alpha + beta * H
				- Choose a prior for alpha, beta, and sigma. These are the unobserved values that we're making a distribution for.
			- Priors matter a lot in complex models. For simple models that can correct themselves easily, not as much.

## **04 - CATEGORIES AND CURVES**

``https://www.youtube.com/watch?v=F0N4b7K_iYQ``
- Short review
	- The geocentric model was an example where it produced good results but wasnt representative of the actual mechanics
	- Kind of similar to the linear regression. Useful to model things that aren't necessarily linear in nature
		- McElreath states it well - a "statistical scaffold" - used to get at statistical estimand, but need to design the linreg with an external, non-linear model in mind.
		- Like the geocentric model, linreg can approximate anything! So we need to design it with a specific owl in mind
- What's new in this lesson?
	- 1) How we draw the statistical models
		- When we have multiple estimatands, it's useful to use multiple estimators.
		- This is why it's important to make a good generator.  
	- 2) How we process the results
		- Often, the estimate we want is not going to show up directly in a table.
		- Need to postprocess multiple values from the posterior distribution.
- What are the new tools introduced in this lesson?
	- Categories
		- As in categorical variables
		- Usually show up as indicators or index variables
	- Curves
		- Linear models can sometimes do extralinear (curved) things.
		- Doesn't mean linear models are obsolete - linear models can handle both
- Categories
	- Lots of causes that are not continuous - categories are discrete, unordered types
	- How do we get around this? Ideally, we want to fit a seperate line (unique distribution) for each category
	- Imagine that we're back in our adult height/weight test. What if we have a data value for male vs. female in each?
		- FIRST! Add it to our DAG. 
			- Before: 	``H -> W``
			- Now: 		``H -> W and S -> H, W``
			- Think: how are H, W, and S causally related? How are they statistically related?
				- Important to ask these questions. If we look at the data, it's not clear whether ``H -> W`` or ``W -> H``, only that there's some relationship.
					- We know scientifically that gaining weight doesn't make you taller. But looking at only data doesn't support this.
				- Interesting note - sex influences weight in two ways - directly, and indirectly (through height.) Remember that there's more than one source of influence.
			- ``H = f sub H (S)`` - "H is a function of S"
			- ``W = f sub W (H, S)`` - "W is a function of H and S"
			- Notice that we don't necessaily *have* to write out the unobserved influences U. Generally it's implied, although you'll want to write out these influences if they're shared.
				- See turtle and lizard example. ``T -> S``, but ``T -> W`` (think food availability, levels of fat storage, etc.)
					- We know it's a compound, so it's not safe to ignore the temperature anymore
		- Think about how we're going to add this to our program
			- Depending on the sex value, use different generator stats and priors.
			- Can use indicator variables (bool for each option) or index variables (one int with multiple values)
				- Natural to see which of these is better coding style. Also better for specifying unique priors and extension to multi-level models
				- Ex. if we want to do color categories, unordered set alpha comprises of ``[a1, a2, ..., ai]`` where each value is a color.
				- Then, when selecting variables, choose from an array for whatever value you want based on the index value of given alpha
				- Ex. ``S[i]`` corresponds to the sex of the i-th person. For person i, we get their index (corresponding to sex category), then get our variable (in this case, intercept alpha)
				- If you use indicator variables, one value becomes the "default" and others become "adjustments" as opposed to everything being on the same field from the getgo.
			- We can find the difference through generative testing. Create data sets with simulated people for each prior and compare the mean weights.
		- We have our posterior distributions. Now what?
			- Remember - posterior distribution is for the average weight. For predicted weights, we need to include standard deviation into our distribution for the posterior predicted weights.
		- We're looking for the difference in posterior distributions, not in the means. So we need to find the difference in distributions. Need to compute contrast!
			- Never legitimate to compare overlap in distributions. You can't tell if things are different or the same based on distribution overlaps.
			- Must compute the **contrast distribution**. This is the actual scientific estimate we're after.
		- What about individual people? (Not the average of infinite people)
			- Take a large number of samples from both predicted posteriors, then take the difference between those two groups.
				- Think of it like so - sample a man and a woman. Find the difference in weight. Do this again and again, n times. This forms a distribution.
				- Negative value indicates woman is heavier than man because the difference is man - woman in weight.
	- This was the causal effect of S on W. Now, how about the direct causal effect of S on W? (Note: this is a distributional estimate)
		- Direct effect = How ONLY sex affects weight. No indirect influence through height - we need to "block" association through H. This means stratify by H.
		- Achieve this by "centering" H - instead of ``mu sub i = alpha + beta * H``, we subtract the average height from our observed height.
			- Makes it so alpha is the average weight of a person with average height
			- With the men and women example, this shows us that the slopes are nearly the same (but not identical!)
		- Then, for each height, simulate individuals and look at the difference in simulated weights. This gives us a posterior distribution in expected weight difference for each height.
			- This gives us the bow tie shape in the video. That's our direct causal effect.
- Curves
	- Not all data is linear. A lot of it can be curved.
	- Like if you look at the data for the full human lifespan, ``H -> W`` is clearly not linear
	- Linear models can easily fit curves, but it's not mechanistically representative.
		- But we knew linreg was like that already. Just have to use it wisely.
	- Technically, 	the model is still linear because it's an additive function of parameters
	- Two popular strategies of fitting curves
		- 1) Polynomials. "Awful."
			- Mu sub i = alpha + beta sub 1 * x sub i + beta sub 2 * x sub i
			- Lots of symmetries that are undesirable. Not scientifically reasonable.
			- Lots of "explosive uncertainty" at the edges
				- See video example for this. There's a little bit at each end of the curve which isn't scientifically justified.
			- Polynomials don't "smooth" the curve locally (don't determine the curve by looking at the curve in dense regions) - only global (accounting for EVERY point)
			- Parabolas **must** curve, and so they do - even when there isn't a very clearly supported fit and something else would do better
				- See video example - a curve fits the data perfectly, but it's not very clearly mechanistically approrpriate.
				- Lots of assumptions are made with a polynomial model that are undesired
		- 2) Splines and generalized additive models. "Less awful."
			- Splines are built from many local functions, then smoothed together to make a single function.
			- Linear models but with some "synthetic variables" (??? sound scary)
			- ``mu sub i = alpha + w1Bsubi,1 + w2Bi,2 + w3Bi,3``
				- ``w`` is the weight of each point. "Like slopes."
				- B is a "spline shape" - synthetic, choose this to determine shape in a particular region.
					- Think of this as a coordinate on the x axis. "B values turn on weights in different regions"
			- Think of multiple "basis functions" adding up to give you a y value.
				- Can make very non-linear functions from linear pieces
			- Adding scientific information helps.
				- Ex. Average weight only increases with height, height increases but levels off with height, etc.
			- Ideally, statistical models have some form as a scientific model, which splines do.
				- Ex. Think phases of human growth. Infancy, childhood, puberty, adulthood all have different growth behaviors
		
## **05 - ELEMENTAL CONFOUNDS**

``https://www.youtube.com/watch?v=mBEA7PKDmiY``
- Certain variables can be correlated with one another without one causing the other. 
- Reminder: Direction of course
	- We have an estimand - parameter being estimated.
	- To make this, we need an estimator - our "set of instructions on how to assemble ingredients (parameters)"
	- This estimator produces an "estimate"
		- Sometimes, the estimate is inaccurate because things happen during the recipe.
		- Good statistical recipes of parameters must defend against confounding.
- There's four of "ye olde elemental confounds" that we'll look at
	- 1) "The Fork"
		- ``X <- Z -> Y``
		- Where Z is a "common cause"
		- When observing X and Y, they appear associated. 
			- Could say that Y "is not independent of" X. This is indicated by the upside down T with a line through it.
			- If you know Y or X, you know something about the other one.
		- Once stratified by Z, there's no association.
			- Meaning that X and Y *are* independent for each (shared) level of Z. 
			- Write this as ``Y (upside down T) X | Z``
			- Simulate Z first, then simulate X and Y based off Z - maybe offset by a coefficient of Z.
			- Sample Z with Bernoulli trials, then generate X and Y seperately with norm distributions. See discrete/continuous examples.
		- Ex. Why do regions of the USA with higher rates of marriage also have higher rates of divorce?
			- Estimand: Causal effect of marriage rate on divorce rate
			- Scientific model
				- Also heavily related: median age of marriage. We have some data regarding that as well.
				- Scientifically, we know that age of marriage has some effect on both marriage rates and divorce rates. ``M <- A -> D``
					- Break the fork by stratifying by A. We need to break the fork to estimate the causal effect of M on D.
					- How do we do this, actually? It's easy where A is discrete (like with Z being {0, 1} in the last example), but it's harder when A is continuous
					- It's the same, really. For every value of A, we look at the assocation between M and D. We just need a continuous function to tell us the output.
			- Statistical model
				- $\mu_i=\alpha+\beta_M * M_i + \beta_A * A_i$
			- Analyze
	- 2) "The Pipe"
		- ``X -> Y -> Z``
	- 3) "The Collider"
		- ``X -> Z <- Y``
	- 4) "The Descendant"
		- ``X -> Z -> Y,A``
